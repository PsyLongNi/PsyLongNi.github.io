<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <!-- <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" /> -->
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Long Ni</title>
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Bootstrap Icons-->
        <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css" rel="stylesheet" />
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Merriweather+Sans:400,700" rel="stylesheet" />
        <link href="https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic" rel="stylesheet" type="text/css" />
        <!-- SimpleLightbox plugin CSS-->
        <link href="https://cdnjs.cloudflare.com/ajax/libs/SimpleLightbox/2.1.0/simpleLightbox.min.css" rel="stylesheet" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light fixed-top py-3" id="mainNav">
            <div class="container px-4 px-lg-5">
                <a class="navbar-brand" href="#page-top">Long Ni</a>
                <button class="navbar-toggler navbar-toggler-right" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto my-2 my-lg-0">
                        <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
                        <li class="nav-item"><a class="nav-link" href="#Publication">Publications</a></li>
                        <!-- <li class="nav-item"><a class="nav-link" href="#portfolio">Portfolio</a></li> -->
                        <li class="nav-item"><a class="nav-link" href="#contact">Contact</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Masthead-->
        <!-- <header class="masthead">
            <div class="container px-4 px-lg-5 h-100">
                <div class="row gx-4 gx-lg-5 h-100 align-items-center justify-content-center text-center">
                    <div class="col-lg-8 align-self-end">
                        <h1 class="text-white font-weight-bold">Long Ni</h1>
                        <hr class="divider" />
                    </div>
                    <div class="col-lg-8 align-self-baseline">
                        <p class="text-white-75 mb-5">working in progress</p>
                    </div>
                </div>
            </div>
        </header> -->

        <header>
          
            <!-- Top header menu containing
                 logo and Navigation bar -->
         <div id="top-header">
                      
                <!-- Logo -->
                <div id="logo">
                    <img src="assets/img/philly1.jpeg" />
                </div>   
         </div> 
        </header>


        <!-- About-->
        <section class="page-section" id="about">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-lg-8">                   
                
                        <h3>About me</h3>
                        <p>I'm Long Ni, a Postdoctoral Fellow in the Department of Psychology and Center for Neural Science at NYU, working with Dr. <a href="https://wp.nyu.edu/landylab/">Michael Landy</a>. I obtained my Ph.D.
                            in Psychology from Upenn, where I was co-supervised by Drs. <a href="https://www.sas.upenn.edu/~astocker/lab/members-files/alan.php">Alan Stocker</a>
                            and <a href="http://burgelab.psych.upenn.edu/">Johannes Burge.</a> My research focuses on visual perception and decision-making, using psychophysics and computational modeling.
                        </p>
                    </div>
                </div>
            </div>
        </section>
         
         <!-- Publication-->
         <section class="page-section" id="Publication">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-lg-8">                
                        <h3>Projects and Publications</h3>

                        <p> <b> - Ni, L., & Stocker, A. A. (2024). Efficient coding of ensemble stimuli relative to a dynamic reference. <i> preprint coming out soon </i> </b> </p>
                        <p>
                        This work is a significant follow-up to our previous paper (<a href="https://www.sciencedirect.com/science/article/pii/S0010027722003237">Ni & Stocker, 2023</a>). 
                        We previously hypothesized that the overweighting of inlying items or robust averaging results from efficient sensory encoding/representation of ensemble stimuli according 
                        to their overall short-term statistics relative to a dynamic reference, reflecting an optimal integration process under the constraints of limited encoding bandwidth. 
                        We demonstrated that this efficient ensemble coding model accounts for various aspects of ensemble coding behavior, including robust averaging in multiple existing datasets. 
                        In this work, we present consistent and compelling evidence for this normative computational framework by validating its two key predictions. We show that 1)
                        the strength of efficient ensemble coding increases as the visual system progressively adapts to a dynamic, approximately Gaussian distribution of ensemble stimuli, resulting in an 
                        increasingly over-weighting of inlying elements; 2) efficient ensemble coding diminishes and the weighting kernel flattens as the visual system adapts to a dynamic uniform stimulus distribution.                         
                        I gave a <i>talk presentation</i> about this work this May at VSS (2024). The abstract can be found here (<a href="https://jov.arvojournals.org/article.aspx?articleid=2801947">Ni & Stocker, 2024</a>). 
                        
                        </p>

                        <p> <b> - Ni, L., & Stocker, A. A. (2024). Dynamics of efficient ensemble coding. <i> bioRxiv </i> 2024-12 </b> </p>
                        <p>
                        This is another extension of our previous work (see <a href="https://www.sciencedirect.com/science/article/pii/S0010027722003237">Ni & Stocker, 2023</a>). We previously showed that `robust averaging' behavior (i.e., downweighting outlier stimuli) in ensemble perception naturally 
                        emerges from a rapid sensory adaptation to the overall ensemble stimulus distribution relative to a changing reference in the experiment. However, the specific mechanism underlying such rapid efficient
                        ensemble coding remains unknown. In this work, we examined the temporal dynamics of efficient ensemble coding by systematically varying the relative timing between the presentation of 
                        the reference and the stimulus ensemble. We found that efficient ensemble coding was only pronounced when the reference and ensemble stimuli were simultaneously
                        presented. We also observed a very interesting asymmetry in performance between conditions wherein the reference preceded versus followed the ensemble stimulus. Specifically, accuracy was higher and reaction time was lower 
                        when the ensemble was first presented. Our normative model well captured both the accuracy and weighting profile observed in each condition. The model results indicate that introducing a temporal 
                        offset between the ensemble and reference stimuli substantially disrupts the dynamic and rapid allocation of coding resource. This disruption decreases decision accuracy and causes a flat weighting profile in ensemble coding. 
                        This works suggests that efficient ensemble coding is established via a fast, instantaneous interaction between the sensory representations of reference and ensemble stimuli. 
                        
                        </p>


                        <p> <b>- Ni, L., & Burge J (2024). Feature-specific divisive normalization improves natural image encoding for depth perception. <i>bioRxiv</i>  611536, 1-42 </b>
                        </p>
                        <p>
                         In this work, we were interested in characterizing the properties of simple-cell like receptive fields that are involved in encoding behaviorally-relevant latent variables, such as binocular disparity. 
                         We examined how response normalization and receptive field properties determine the fidelity with which binocular disparity is encoded in natural scenes.
                         Specifically, we used nonlinear-linear subunit modeling framework to simulate the responses of binocular receptive fields to binocular disparity from natural stereo-images with groundtruth disparity at each pixel. 
                         We then computed encoding fidelity, measured by Fisher information, of different levels of binocular disparity by receptive fields with different preferred properties 
                         (e.g., spatial frequency, phase shift). We find that as compared to the broadband-normalization, narrow-band (or feature-specific) divisive normalization improves encoding fidelity of binocular disparity, and the improvement grows with the neural population size.
                         Moreover, we find that the usefulness of different spatial frequencies of receptive fields for encoding binocular disparity in natural scenes depends on the disparity amplitude. 
                         
                         Though not included in the current manuscript, we also examined how external stimulus variability and internal encoding noise jointly determine the binocular receptive 
                         field properties that are most useful for encoding binocular disparity in natural scenes. These results were presented at VSS2022 (see <a href="https://drive.google.com/open?id=1vfs6uJ0JfiEohY6MxcWBWceSI9IvKZjy&authuser=nilong%40sas.upenn.edu&usp=drive_fs">our Poster</a> here). 
                         
                        </p>


                    
                        <p> <b>- Ni, L., & Ma, W.J. (2024). A computational approach to the N-back task. <i>Scientific Reports, 14</i> (1), 1-19 </b> </p>
                        <p>
                        I started this project with Dr. Wei Ji Ma several years ago. Back then, we were interested in dissociating the sources of interference in the <i>N</i>-back task, a widely used working memory paradigm. 
                        Performance in the classic <i>N</i>-back task is known to suffer from interference from the intervening items in the sequence. Unfortunately, the categorical stimuli (e.g., letters and numbers) used
                        in the classic <i>N</i>-back task precluded researchers from measuring interference as a function of probe-distractor similarity and characterizing the impact of different items in the sequence on task performance. 
                        So the first thing we did was to create a new variant of the classic <i>N</i>-back task. In the new variant, we used continuous stimuli such as color and orientation, which 
                        allowed us to measure similarity-based interference. With the desirable dataset collected from this analog <i>N</i>-back task, we next created Bayesian models of interference to dissociate two
                        general sources of interference: pooling and substitution. Crucially, each of our interference models differs from the optimal non-interference model in only component only, allowing us to pinpoint the locus of 
                        interference. We found that interference in the analog <i>N</i>-back task most likely results from confusion between the target and distractor stimuli within the sequence. 
                         Lately, we also demonstrated that our models can be generalized to other domains and help dissociate
                        the sources of interference beyond the <i>N</i>-back task. 
                        </p>

                        <p> <b>- Ni, L., & Stocker, A. A. (2023). Efficient sensory encoding predicts robust averaging. <i>Cognition, 232,</i> 105334.</b> </p>
                        <p>
                        This work started with a behavioral phenomenon in ensemble perception: observers tend to assign higher weights to stimuli with features close to the set mean
                        and lower weights to those with features away from the set mean. This phenomenon, termed robust averaging, has been reported in ensemble perception of both low-level visual features (e.g., color, orientation, and shape) and high-level visual stimuli (e.g., faces).
                        Robust averaging has been taken as evidence for the non-optimal integration of sensory information. In this work, however, we show that robust averaging naturally emerges 
                        from an optimal integration process when sensory encoding is efficiently adapted to the overall ensemble statistics in the experiment relative to a reference. The core idea of efficient coding is that the limited sensory bandwidth of a perceptual system
                        is optimally allocated according to the statistical regularities of the sensory input such that stimuli that occur frequently are more accurately represented than those occurring rarely. Prior work from the lab has shown that
                        efficient representations according to the natural, long-term stimulus statistics can 
                        account for various aspects of perceptual bias and variability that traditional Bayesian observer models are not able to explain (Wei and Stocker, 2012, 2015).
                        Our work suggests that efficient sensory encoding can operate on short time-scales to improve overall decision performance. (see our <a href="https://www.sciencedirect.com/science/article/pii/S0010027722003237">paper</a> here.)
                        We recently extended this exciting line of work by providing compelling empirical and computational evidence that our visual system can rapidly form efficient representation of ensemble stimuli according to their overall statistics relative to a dynamic reference. 
                        I will be giving a <i>talk presentation</i> about the latest development of this work this May at VSS (2024), with the title <i>'Efficient coding of ensemble stimuli relative to a dynamic reference'.</i>
                        </p>
                        
                        <p>---------- non-recent work -----------</p>                   
                        <p> <b> - Yu, W., Ni, L., Zhang, J., Zheng, W., & Liu, Y. (2023). No need to integrate action information during coarse semantic processing of man-made tools. 
                            <i>Psychonomic Bulletin & Review </i>, pages 1–10, 2023.</b> </p>
                        <p> <b> - Gong, J., Yu, W., Ni, L., Jiao, Y., Liu, Y., Fu, X., & Xu, Y. (2020, October). &quot I can't name it, but I can perceive it &quot Conceptual and Operational Design of” Tactile Accuracy” Assisting Tactile Image Cognition. 
                        <i>In The 22nd International ACM SIGACCESS Conference on Computers and Accessibility</i> (pp. 1-12).</b> </p>
                        <p> <b>- Ni, L., Liu, Y., & Yu, W. (2019). The dominant role of functional action representation in object recognition. <i>Experimental Brain Research, 237</i>(2), 363-375.</b> </p>
                        <p> <b>- Ni, L., Liu, Y., Yu, W., & Fu, X. (2019). The China Image Set (CIS): A new set of 551 colored photos with Chinese norms for 12 psycholinguistic variables. <i>Frontiers in Psychology, 10</i>, 2631. </b> </p>
                        <p> <b>- Li, L., Ni, L., Lappe, M., Niehorster, D. C., & Sun, Q. (2018). No special treatment of independent object motion for heading perception. <i>Journal of Vision, 18(4)</i>, 1-16. </b> </p>


                    </div>
                </div>
            </div>
        </section>

        <!-- Services-->
        <!-- <section class="page-section" id="services">
            <div class="container px-4 px-lg-5">
                <h2 class="text-center mt-0">At Your Service</h2>
                <hr class="divider" />
                <div class="row gx-4 gx-lg-5">
                    <div class="col-lg-3 col-md-6 text-center">
                        <div class="mt-5">
                            <div class="mb-2"><i class="bi-gem fs-1 text-primary"></i></div>
                            <h3 class="h4 mb-2">Sturdy Themes</h3>
                            <p class="text-muted mb-0">Our themes are updated regularly to keep them bug free!</p>
                        </div>
                    </div>
                    <div class="col-lg-3 col-md-6 text-center">
                        <div class="mt-5">
                            <div class="mb-2"><i class="bi-laptop fs-1 text-primary"></i></div>
                            <h3 class="h4 mb-2">Up to Date</h3>
                            <p class="text-muted mb-0">All dependencies are kept current to keep things fresh.</p>
                        </div>
                    </div>
                    <div class="col-lg-3 col-md-6 text-center">
                        <div class="mt-5">
                            <div class="mb-2"><i class="bi-globe fs-1 text-primary"></i></div>
                            <h3 class="h4 mb-2">Ready to Publish</h3>
                            <p class="text-muted mb-0">You can use this design as is, or you can make changes!</p>
                        </div>
                    </div>
                    <div class="col-lg-3 col-md-6 text-center">
                        <div class="mt-5">
                            <div class="mb-2"><i class="bi-heart fs-1 text-primary"></i></div>
                            <h3 class="h4 mb-2">Made with Love</h3>
                            <p class="text-muted mb-0">Is it really open source if it's not made with love?</p>
                        </div>
                    </div>
                </div>
            </div>
        </section> -->
       
        <!-- Call to action-->
       <!--   <section class="page-section bg-dark text-white">
            <div class="container px-4 px-lg-5 text-center">
                <h2 class="mb-4">Free Download at Start Bootstrap!</h2>
                <a class="btn btn-light btn-xl" href="https://startbootstrap.com/theme/creative/">Download Now!</a>
            </div>
        </section> --> 
        <!-- Contact-->
        <section class="page-section" id="contact">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-lg-8">                
                        <h3>Contact me</h3>
                        <p>Email: ln957@nyu.edu</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Footer-->
        <footer class="bg-light py-5">
            <div class="container px-4 px-lg-5"><div class="small text-center text-muted">last updated -Dec/27/2024 by Long Ni</div></div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- SimpleLightbox plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/SimpleLightbox/2.1.0/simpleLightbox.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
        <!-- * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *-->
        <!-- * *                               SB Forms JS                               * *-->
        <!-- * * Activate your form at https://startbootstrap.com/solution/contact-forms * *-->
        <!-- * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *-->
        <script src="https://cdn.startbootstrap.com/sb-forms-latest.js"></script>
    </body>
</html>
